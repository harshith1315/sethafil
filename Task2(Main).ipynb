{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 langchain langchain-community faiss-cpu huggingface-hub transformers numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7uE2Qbs9DpT",
        "outputId": "9a24c1b0-dd83-4734-8b83-cc6ae8904305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0.post1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.26.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.25)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.3)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2024.10.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ffgF5wk7Vae"
      },
      "outputs": [],
      "source": [
        "# import requests\n",
        "# from bs4 import BeautifulSoup\n",
        "\n",
        "# def scrape_website(url):\n",
        "#     response = requests.get(url)\n",
        "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
        "#     # Extract relevant content (e.g., mission statements)\n",
        "#     # This could be a targeted section or just the main body of text\n",
        "#     content = soup.get_text()  # Or find a specific tag that contains the mission\n",
        "#     return content\n",
        "\n",
        "# # Scrape all four websites\n",
        "# uchicago_content = scrape_website(\"https://www.uchicago.edu/\")\n",
        "# washington_content = scrape_website(\"https://www.washington.edu/\")\n",
        "# stanford_content = scrape_website(\"https://www.stanford.edu/\")\n",
        "# und_content = scrape_website(\"https://und.edu/\")\n",
        "\n",
        "# # Combine all the content into one list of documents\n",
        "# documents = [\n",
        "#     {\"content\": uchicago_content, \"source\": \"University of Chicago\"},\n",
        "#     {\"content\": washington_content, \"source\": \"University of Washington\"},\n",
        "#     {\"content\": stanford_content, \"source\": \"Stanford University\"},\n",
        "#     {\"content\": und_content, \"source\": \"University of North Dakota\"}\n",
        "# ]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "\n",
        "# # Split the scraped content into smaller chunks\n",
        "# documents_chunks = []\n",
        "# for doc in documents:\n",
        "#     chunks = text_splitter.split_text(doc['content'])\n",
        "#     for chunk in chunks:\n",
        "#         documents_chunks.append({\"content\": chunk, \"source\": doc['source']})\n"
      ],
      "metadata": {
        "id": "qk7H8gdh9Tb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "# from langchain_community.vectorstores import FAISS\n",
        "# from langchain.docstore.document import Document\n",
        "\n",
        "# # Wrap the documents in the Document class\n",
        "# documents_chunks = []\n",
        "# for doc in documents:\n",
        "#     chunks = text_splitter.split_text(doc['content'])\n",
        "#     for chunk in chunks:\n",
        "#         documents_chunks.append(Document(page_content=chunk, metadata={\"source\": doc['source']}))\n",
        "\n",
        "# # Define the embedding model\n",
        "# huggingface_embeddings = HuggingFaceBgeEmbeddings(\n",
        "#     model_name=\"BAAI/bge-small-en-v1.5\",  # You can use a different model here\n",
        "#     model_kwargs={'device': 'cpu'},\n",
        "#     encode_kwargs={'normalize_embeddings': True}\n",
        "# )\n",
        "\n",
        "# # Vectorize the documents\n",
        "# vectorstore = FAISS.from_documents(documents_chunks, huggingface_embeddings)\n"
      ],
      "metadata": {
        "id": "6FrA7OKW9ckE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.environ['HUGGINGFACEHUB_API_TOKEN']=\"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\""
      ],
      "metadata": {
        "id": "0qRT-ska-doq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define the query\n",
        "# query = \"What is the mission of standford?\"\n",
        "\n",
        "# # Perform the similarity search\n",
        "# relevant_documents = vectorstore.similarity_search(query, k=3)\n",
        "# #\n",
        "# # Print the retrieved content\n",
        "# # Print the retrieved content\n",
        "# for doc in relevant_documents:\n",
        "#     print(f\"Source: {doc.metadata['source']}\")\n",
        "#     print(f\"Content: {doc.page_content}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "cNYniWLG93Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hf = HuggingFacePipeline.from_model_id(\n",
        "#     model_id=\"gpt2\",\n",
        "#     task=\"text-generation\",\n",
        "#     pipeline_kwargs={\"temperature\": 0.1, \"max_new_tokens\": 300}\n",
        "# )\n"
      ],
      "metadata": {
        "id": "cVS4CDi4BGMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt_template = \"\"\"summarize the given content\n",
        "# Question: {question}\n",
        "# Answer:\"\"\"\n",
        "# PROMPT = PromptTemplate(\n",
        "#     template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        "# )\n",
        "\n",
        "# # Create an LLMChain using HuggingFacePipeline and the prompt template\n",
        "# llm_chain = LLMChain(llm=hf, prompt=PROMPT)\n"
      ],
      "metadata": {
        "id": "kS1YGAZEBGJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "from transformers import pipeline\n",
        "import os\n",
        "\n",
        "# Set up Hugging Face API token for embeddings\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = \"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\"\n",
        "\n",
        "# Define the function to scrape website content\n",
        "def scrape_website(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    content = soup.get_text()  # Extract the text content\n",
        "    return content\n",
        "\n",
        "# Scrape website content\n",
        "documents = [\n",
        "    {\"content\": scrape_website(\"https://www.uchicago.edu/\"), \"source\": \"University of Chicago\"},\n",
        "    {\"content\": scrape_website(\"https://www.washington.edu/\"), \"source\": \"University of Washington\"},\n",
        "    {\"content\": scrape_website(\"https://www.stanford.edu/\"), \"source\": \"Stanford University\"},\n",
        "    {\"content\": scrape_website(\"https://und.edu/\"), \"source\": \"University of North Dakota\"}\n",
        "]\n",
        "\n",
        "# Split the content into smaller chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "documents_chunks = []\n",
        "for doc in documents:\n",
        "    chunks = text_splitter.split_text(doc['content'])\n",
        "    for chunk in chunks:\n",
        "        documents_chunks.append(Document(page_content=chunk, metadata={\"source\": doc['source']}))\n",
        "\n",
        "# Define the embedding model\n",
        "huggingface_embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
        "    model_kwargs={'device': 'cpu'},\n",
        "    encode_kwargs={'normalize_embeddings': True}\n",
        ")\n",
        "\n",
        "# Create the vector store\n",
        "vectorstore = FAISS.from_documents(documents_chunks, huggingface_embeddings)\n",
        "\n",
        "# Define the query\n",
        "query = \"tell me about North Dakota university?\"\n",
        "\n",
        "# Perform similarity search\n",
        "relevant_documents = vectorstore.similarity_search(query, k=3)\n",
        "\n",
        "# Summarization pipeline from Hugging Face\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Summarize the retrieved content\n",
        "all_content = \" \".join([doc.page_content for doc in relevant_documents])\n",
        "summary = summarizer(all_content, max_length=150, min_length=50, do_sample=False)\n",
        "\n",
        "# Print the summarized output\n",
        "print(\"Summary of the Mission Statement:\")\n",
        "print(summary[0]['summary_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60qnE7tnA46p",
        "outputId": "3fa716fd-40ca-4eec-a26e-45048ff532a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary of the Mission Statement:\n",
            "The University of North Dakota is the state's oldest and largest university. We offer 225+ highly accredited on-campus and online degrees. Leaders can come from anywhere, but they go to UND. Find out more at UND's website: http://www.unD.edu.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q0r5zcL6DKJ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}